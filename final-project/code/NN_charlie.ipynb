{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charliecheng/anaconda3/lib/python3.11/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3314, 620) (3314, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from train_test_split import site_train_val_test_split\n",
    "\n",
    "sns.set_palette(\"colorblind\")\n",
    "df = pd.read_csv('../data/merged_data_cleaned.csv', low_memory=False)\n",
    "target = pd.read_csv('../data/target.csv')\n",
    "target.columns = target.columns.str.lower()\n",
    "\n",
    "# Rename target[\"csfractures\"] to target[\"csi\"]\n",
    "target.rename(columns={\"csfractures\": \"csi\"}, inplace=True)\n",
    "\n",
    "print(df.shape, target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate any column is object type\n",
    "df = df.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Change -1 to 0\n",
    "df = df.replace(-1, 0)\n",
    "\n",
    "# Drop sectiongcsavailable\n",
    "df = df.drop(columns='sectiongcsavailable')\n",
    "df = df.select_dtypes(exclude=['object'])\n",
    "target = target.set_index('studysubjectid').loc[df['studysubjectid']].reset_index()\n",
    "df_feat = df.drop(columns=[\"site\", \"caseid\", \"studysubjectid\"])\n",
    "df.drop(columns=[\"ageinyears\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "df.fillna(0, inplace=True)\n",
    "nan_counts = df.isnull().sum()\n",
    "print(nan_counts[nan_counts > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df, train_target, val_target, test_target = site_train_val_test_split(df, target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.3799, Val Loss: 0.3307, Val Accuracy: 0.8990\n",
      "F1 score: 0.8512414800389484\n",
      "Epoch 2/10, Train Loss: 0.3245, Val Loss: 0.3314, Val Accuracy: 0.8990\n",
      "F1 score: 0.8512414800389484\n",
      "Epoch 3/10, Train Loss: 0.3258, Val Loss: 0.3251, Val Accuracy: 0.8990\n",
      "F1 score: 0.8512414800389484\n",
      "Epoch 4/10, Train Loss: 0.3202, Val Loss: 0.3257, Val Accuracy: 0.8990\n",
      "F1 score: 0.8512414800389484\n",
      "Epoch 5/10, Train Loss: 0.3157, Val Loss: 0.3273, Val Accuracy: 0.8990\n",
      "F1 score: 0.8512414800389484\n",
      "Epoch 6/10, Train Loss: 0.3178, Val Loss: 0.3278, Val Accuracy: 0.8990\n",
      "F1 score: 0.8512414800389484\n",
      "Epoch 7/10, Train Loss: 0.3154, Val Loss: 0.3315, Val Accuracy: 0.8990\n",
      "F1 score: 0.8512414800389484\n",
      "Epoch 8/10, Train Loss: 0.3162, Val Loss: 0.3304, Val Accuracy: 0.8990\n",
      "F1 score: 0.8512414800389484\n",
      "Epoch 9/10, Train Loss: 0.3232, Val Loss: 0.3289, Val Accuracy: 0.8990\n",
      "F1 score: 0.8512414800389484\n",
      "Epoch 10/10, Train Loss: 0.3164, Val Loss: 0.3309, Val Accuracy: 0.8990\n",
      "F1 score: 0.8512414800389484\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, units, dropout_list, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.units = units\n",
    "        for i, u in enumerate(units):\n",
    "            setattr(self, f'fc{i}', torch.nn.Linear(input_size, u))\n",
    "            setattr(self, f'dropout{i}', torch.nn.Dropout(p=dropout_list[i]))\n",
    "            setattr(self, f'relu{i}', torch.nn.ReLU())\n",
    "            # setattr(self, f'batchnorm{i}', torch.nn.BatchNorm1d(u))\n",
    "            input_size = u\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.fc_out = torch.nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.units)):\n",
    "            x = getattr(self, f'fc{i}')(x)\n",
    "            x = getattr(self, f'dropout{i}')(x)\n",
    "            x = getattr(self, f'relu{i}')(x)\n",
    "            # x = getattr(self, f'batchnorm{i}')(x)\n",
    "        x = self.fc_out(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = MLP(input_size=train_df.shape[1], units=[32, 32, 8], dropout_list=[0.01, 0.01, 0.01], output_size=1)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "\n",
    "\n",
    "X_train_tensor = torch.tensor(train_df, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(train_target, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "X_val_tensor = torch.tensor(val_df, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(val_target, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "X_test_tensor = torch.tensor(test_df, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(test_target, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=100):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            # outputs = torch.sigmoid(outputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            # outputs = torch.sigmoid(outputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        val_accuracy = accuracy_score(y_val_tensor, model(X_val_tensor).detach().numpy() > 0.5)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "        print(f'F1 score: {f1_score(y_val_tensor, model(X_val_tensor).detach().numpy() > 0.5, average=\"weighted\")}')\n",
    "    return train_losses, val_losses\n",
    "\n",
    "train_losses, val_losses = train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
